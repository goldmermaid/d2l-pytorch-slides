{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "grave-parker",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Image Classification Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laughing-response",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This lecture will be a bit different different than others, and we will dive deep into an image classification dataset.\n",
    "\n",
    "Now imagine you are a ML scientist back in 20 years ago and you are thinking of a dataset for benchmarking image classification models.\n",
    "Then the first dataset comes to your mind is highly likely to be the MNIST dataset. It was created by LeCun and his colleagues in 1998, and it is one of the widely used dataset for image classification.\n",
    "\n",
    "However, as of today, even a simple model can achieve a classification accuracy over 95%, making it unsuitable for distinguishing between stronger models and the weaker ones. \n",
    "So Today, MNIST serves as more of a sanity check dataset than as a benchmark one. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bottom-international",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T22:49:42.427367Z",
     "start_time": "2021-01-27T22:49:22.149056Z"
    },
    "execution": {
     "iopub.execute_input": "2021-01-27T22:46:15.596835Z",
     "iopub.status.busy": "2021-01-27T22:46:15.595763Z",
     "iopub.status.idle": "2021-01-27T22:46:16.806050Z",
     "shell.execute_reply": "2021-01-27T22:46:16.805357Z"
    },
    "origin_pos": 2,
    "slideshow": {
     "slide_type": "-"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from d2l import torch as d2l\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils import data\n",
    "\n",
    "d2l.use_svg_display() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-location",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "To up the ante just a bit, we will focus this lecture in a  comparatively complex dataset released in 2017 -- Fashion-MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-illustration",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can download and read the Fashion-MNIST dataset into memory via the build-in functions in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handy-greek",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\n",
    "`ToTensor` converts the image data from PIL type to 32-bit floating point tensors. It divides all numbers by 255 so that all pixel values are between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rising-settlement",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T22:49:51.693175Z",
     "start_time": "2021-01-27T22:49:42.429467Z"
    },
    "execution": {
     "iopub.execute_input": "2021-01-27T22:46:16.812371Z",
     "iopub.status.busy": "2021-01-27T22:46:16.811250Z",
     "iopub.status.idle": "2021-01-27T22:46:16.856883Z",
     "shell.execute_reply": "2021-01-27T22:46:16.856176Z"
    },
    "origin_pos": 6,
    "slideshow": {
     "slide_type": "-"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "trans = transforms.ToTensor()\n",
    "mnist_train = torchvision.datasets.FashionMNIST(\n",
    "    root=\"../data\", train=True, transform=trans, download=True)\n",
    "mnist_test = torchvision.datasets.FashionMNIST(\n",
    "    root=\"../data\", train=False, transform=trans, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corporate-botswana",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Fashion-MNIST consists of images from 10 categories, each of the category has  6000 images in the training set and 1000 images in the test set. \n",
    "\n",
    "For those of you who dont know \"what is a test dataset?\" it is used for evaluating model performance, but not for training. \n",
    "\n",
    "So let's print the total number of images. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greatest-schema",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In total, we have 60,000 images in the training set and 10,000 images in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "declared-backing",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T22:49:51.701121Z",
     "start_time": "2021-01-27T22:49:51.695465Z"
    },
    "execution": {
     "iopub.execute_input": "2021-01-27T22:46:16.864081Z",
     "iopub.status.busy": "2021-01-27T22:46:16.863306Z",
     "iopub.status.idle": "2021-01-27T22:46:16.867089Z",
     "shell.execute_reply": "2021-01-27T22:46:16.867680Z"
    },
    "origin_pos": 9,
    "slideshow": {
     "slide_type": "-"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "len(mnist_train), len(mnist_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "given-tanzania",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's take a look of the shape of each image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compound-funds",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\n",
    "For each image, the height and width both contain 28 pixels. \n",
    "While since this is a grayscale image dataset, so the number of \"color\" channels is 1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "christian-locking",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T22:49:51.707153Z",
     "start_time": "2021-01-27T22:49:51.703304Z"
    },
    "execution": {
     "iopub.execute_input": "2021-01-27T22:46:16.872101Z",
     "iopub.status.busy": "2021-01-27T22:46:16.871170Z",
     "iopub.status.idle": "2021-01-27T22:46:16.875455Z",
     "shell.execute_reply": "2021-01-27T22:46:16.876043Z"
    },
    "origin_pos": 12,
    "slideshow": {
     "slide_type": "-"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "mnist_train[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "external-chassis",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We can also get the categories of the Fashion-MNIST datasets, which contains t-shirt, trousers, pullover, dress, coat, sandal, shirt, sneaker, bag, and ankle boot. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driven-mortgage",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The function `get_fashion_mnist_labels` returns text labels for the Fashion-MNIST dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southeast-cancer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T22:49:51.712206Z",
     "start_time": "2021-01-27T22:49:51.708742Z"
    },
    "execution": {
     "iopub.execute_input": "2021-01-27T22:46:16.881356Z",
     "iopub.status.busy": "2021-01-27T22:46:16.880499Z",
     "iopub.status.idle": "2021-01-27T22:46:16.883388Z",
     "shell.execute_reply": "2021-01-27T22:46:16.882772Z"
    },
    "origin_pos": 14,
    "slideshow": {
     "slide_type": "-"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def get_fashion_mnist_labels(labels):  #@save\n",
    "    \"\"\"Return text labels for the Fashion-MNIST dataset.\"\"\"\n",
    "    text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n",
    "                   'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
    "    return [text_labels[int(i)] for i in labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daily-bottle",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And the function `show_images` visualizes these examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quarterly-wichita",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T22:49:51.719012Z",
     "start_time": "2021-01-27T22:49:51.713877Z"
    },
    "execution": {
     "iopub.execute_input": "2021-01-27T22:46:16.890761Z",
     "iopub.status.busy": "2021-01-27T22:46:16.889692Z",
     "iopub.status.idle": "2021-01-27T22:46:16.892473Z",
     "shell.execute_reply": "2021-01-27T22:46:16.891971Z"
    },
    "origin_pos": 17,
    "slideshow": {
     "slide_type": "-"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):  #@save\n",
    "    \"\"\"Plot a list of images.\"\"\"\n",
    "    figsize = (num_cols * scale, num_rows * scale)\n",
    "    _, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "    for i, (ax, img) in enumerate(zip(axes, imgs)):\n",
    "        if torch.is_tensor(img):\n",
    "            # Tensor Image\n",
    "            ax.imshow(img.numpy())\n",
    "        else:\n",
    "            # PIL Image\n",
    "            ax.imshow(img)\n",
    "        ax.axes.get_xaxis().set_visible(False)\n",
    "        ax.axes.get_yaxis().set_visible(False)\n",
    "        if titles:\n",
    "            ax.set_title(titles[i])\n",
    "    return axes  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clean-paragraph",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Here are some visualization of the images and their corresponding text labels for the first few examples in the training dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anonymous-middle",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T22:49:52.238372Z",
     "start_time": "2021-01-27T22:49:51.720697Z"
    },
    "execution": {
     "iopub.execute_input": "2021-01-27T22:46:16.897519Z",
     "iopub.status.busy": "2021-01-27T22:46:16.896506Z",
     "iopub.status.idle": "2021-01-27T22:46:17.710596Z",
     "shell.execute_reply": "2021-01-27T22:46:17.711188Z"
    },
    "origin_pos": 20,
    "slideshow": {
     "slide_type": "slide"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "X, y = next(iter(data.DataLoader(mnist_train, batch_size=18)))\n",
    "show_images(X.reshape(18, 28, 28), 2, 9, titles=get_fashion_mnist_labels(y));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dressed-annex",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reading a Minibatch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "social-organization",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "To make our life easier when reading from the training and test sets, we use the built-in data iterator rather than creating one from scratch. Recall that at each iteration, a data loader reads a minibatch of data with size `batch_size` at each time. \n",
    "\n",
    "We also randomly shuffle the examples through the iterator, while note that we only need to shuffle for the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endless-threshold",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T22:49:52.243971Z",
     "start_time": "2021-01-27T22:49:52.241214Z"
    },
    "execution": {
     "iopub.execute_input": "2021-01-27T22:46:17.716044Z",
     "iopub.status.busy": "2021-01-27T22:46:17.715211Z",
     "iopub.status.idle": "2021-01-27T22:46:17.718088Z",
     "shell.execute_reply": "2021-01-27T22:46:17.717478Z"
    },
    "origin_pos": 24,
    "slideshow": {
     "slide_type": "-"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "def get_dataloader_workers():  #@save\n",
    "    \"\"\"Use 4 processes to read the data.\"\"\"\n",
    "    return 4\n",
    "\n",
    "train_iter = data.DataLoader(mnist_train, batch_size, shuffle=True,\n",
    "                             num_workers=get_dataloader_workers())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "third-hygiene",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "How long does it take to read the full training dataset? Let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superior-trance",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T22:49:55.214268Z",
     "start_time": "2021-01-27T22:49:52.245705Z"
    },
    "execution": {
     "iopub.execute_input": "2021-01-27T22:46:17.722603Z",
     "iopub.status.busy": "2021-01-27T22:46:17.721656Z",
     "iopub.status.idle": "2021-01-27T22:46:19.317564Z",
     "shell.execute_reply": "2021-01-27T22:46:19.318175Z"
    },
    "origin_pos": 27,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "timer = d2l.Timer()\n",
    "for X, y in train_iter:\n",
    "    continue\n",
    "f'{timer.stop():.2f} sec'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-capacity",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Putting All Things Together\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extreme-toolbox",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Now it is the time to put all the things together, we can define this `load_data_fashion_mnist` function that reads the Fashion-MNIST dataset. It returns the data iterators for both the training set and validation set. In addition, it accepts an optional argument to resize images to another shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "combined-tongue",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T22:49:55.220734Z",
     "start_time": "2021-01-27T22:49:55.215977Z"
    },
    "execution": {
     "iopub.execute_input": "2021-01-27T22:46:19.324924Z",
     "iopub.status.busy": "2021-01-27T22:46:19.324333Z",
     "iopub.status.idle": "2021-01-27T22:46:19.326526Z",
     "shell.execute_reply": "2021-01-27T22:46:19.326992Z"
    },
    "origin_pos": 30,
    "slideshow": {
     "slide_type": "-"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def load_data_fashion_mnist(batch_size, resize=None):  #@save\n",
    "    \"\"\"Download the Fashion-MNIST dataset and then load it into memory.\"\"\"\n",
    "    trans = [transforms.ToTensor()]\n",
    "    if resize:\n",
    "        trans.insert(0, transforms.Resize(resize))\n",
    "    trans = transforms.Compose(trans) \n",
    "    mnist_train = torchvision.datasets.FashionMNIST(\n",
    "        root=\"../data\", train=True, transform=trans, download=True)\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(\n",
    "        root=\"../data\", train=False, transform=trans, download=True)\n",
    "    return (data.DataLoader(mnist_train, batch_size, shuffle=True,\n",
    "                            num_workers=get_dataloader_workers()),\n",
    "            data.DataLoader(mnist_test, batch_size, shuffle=False,\n",
    "                            num_workers=get_dataloader_workers()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporate-precipitation",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Let's test the image resizing feature of the `load_data_fashion_mnist function` by specifying the resize argument to be 64 x 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dated-processing",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T22:49:56.040767Z",
     "start_time": "2021-01-27T22:49:55.222676Z"
    },
    "execution": {
     "iopub.execute_input": "2021-01-27T22:46:19.331151Z",
     "iopub.status.busy": "2021-01-27T22:46:19.330583Z",
     "iopub.status.idle": "2021-01-27T22:46:19.443168Z",
     "shell.execute_reply": "2021-01-27T22:46:19.443781Z"
    },
    "origin_pos": 33,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "train_iter, test_iter = load_data_fashion_mnist(32, resize=64)\n",
    "for X, y in train_iter:\n",
    "    print(X.shape, X.dtype, y.shape, y.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-equation",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Great job! Now the Fashion-MNIST dataset is ready-to-be-trained, and we will move on to the more complex model training in the next section."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "rise": {
   "autolaunch": true,
   "enable_chalkboard": true,
   "overlay": "<div class='my-top-right'><img height=80px src='http://d2l.ai/_static/logo-with-text.png'/></div><div class='my-top-left'></div>"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
