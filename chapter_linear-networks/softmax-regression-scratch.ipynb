{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "valid-traffic",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Implementation of Softmax Regression from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "centered-orchestra",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T22:57:43.750508Z",
     "iopub.status.busy": "2021-01-27T22:57:43.749942Z",
     "iopub.status.idle": "2021-01-27T22:57:44.945808Z",
     "shell.execute_reply": "2021-01-27T22:57:44.944980Z"
    },
    "origin_pos": 2,
    "slideshow": {
     "slide_type": "-"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "from d2l import torch as d2l\n",
    "import torch\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comfortable-chancellor",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "we believe that softmax regression is similarly fundamental and you ought to know the gory details of how to implement it yourself. \n",
    "\n",
    "We will work with the Fashion-MNIST dataset, just introduced in Section 3.5 setting up a data iterator with batch size 256."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flush-problem",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's set up a data iterator with batch size 256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sought-observer",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T22:57:44.949789Z",
     "iopub.status.busy": "2021-01-27T22:57:44.949191Z",
     "iopub.status.idle": "2021-01-27T22:57:44.994229Z",
     "shell.execute_reply": "2021-01-27T22:57:44.993541Z"
    },
    "origin_pos": 4,
    "slideshow": {
     "slide_type": "-"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "underlying-speaking",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We flatten each image, treating them as vectors of length 784. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuck-thailand",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Recall that in softmax regression, we have as many outputs as there are classes. Because our dataset has 10 classes, our network will have an output dimension of 10. Consequently, our weights will constitute a  784×10  matrix and the biases will constitute a  1×10  row vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brazilian-information",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T22:57:44.999221Z",
     "iopub.status.busy": "2021-01-27T22:57:44.998272Z",
     "iopub.status.idle": "2021-01-27T22:57:45.001167Z",
     "shell.execute_reply": "2021-01-27T22:57:45.000599Z"
    },
    "origin_pos": 7,
    "slideshow": {
     "slide_type": "-"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "num_inputs = 784\n",
    "num_outputs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "employed-manhattan",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We initialize the weights W with Gaussian noise and our biases to take the initial value 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welsh-carolina",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "W = torch.normal(0, 0.01, size=(num_inputs, num_outputs), requires_grad=True)\n",
    "b = torch.zeros(num_outputs, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "remarkable-proportion",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Before implementing the softmax regression model, let us briefly review how the sum operator works along specific dimensions in a tensor, as discussed in Section 2.3.6 and Section 2.3.6.1. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooked-drawing",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Given a matrix X we can sum over all elements (by default) or only over elements in the same axis, i.e., the same column (axis 0) or the same row (axis 1). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interstate-reservation",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Note that if X is a tensor with shape (2, 3) and we sum over the columns, the result will be a vector with shape (3,). When invoking the sum operator, we can specify to keep the number of axes in the original tensor, rather than collapsing out the dimension that we summed over. This will result in a two-dimensional tensor with shape (1, 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simplified-tactics",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T22:57:45.005205Z",
     "iopub.status.busy": "2021-01-27T22:57:45.004657Z",
     "iopub.status.idle": "2021-01-27T22:57:45.011903Z",
     "shell.execute_reply": "2021-01-27T22:57:45.011331Z"
    },
    "origin_pos": 10,
    "slideshow": {
     "slide_type": "-"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "X = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "X.sum(0, keepdim=True), X.sum(1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forty-oasis",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We are now ready to implement the softmax operation. Recall that softmax consists of three steps: i) we exponentiate each term (using exp); ii) we sum over each row (we have one row per example in the batch) to get the normalization constant for each example; iii) we divide each row by its normalization constant, ensuring that the result sums to 1. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preceding-progressive",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Softmax operation expressed as an equation:\n",
    "\n",
    "$$\\mathrm{softmax}(\\mathbf{X})_{ij} = \\frac{\\exp(\\mathbf{X}_{ij})}{\\sum_k \\exp(\\mathbf{X}_{ik})}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rubber-spokesman",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T22:57:45.015849Z",
     "iopub.status.busy": "2021-01-27T22:57:45.015305Z",
     "iopub.status.idle": "2021-01-27T22:57:45.017865Z",
     "shell.execute_reply": "2021-01-27T22:57:45.017300Z"
    },
    "origin_pos": 14,
    "slideshow": {
     "slide_type": "-"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    X_exp = torch.exp(X)\n",
    "    partition = X_exp.sum(1, keepdim=True)\n",
    "    return X_exp / partition  # The broadcasting mechanism is applied here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educational-printing",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "As you can see, for any random input, we turn each element into a non-negative number. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "administrative-decision",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Moreover, each row sums up to 1, as is required for a probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "theoretical-variation",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T22:57:45.021588Z",
     "iopub.status.busy": "2021-01-27T22:57:45.020845Z",
     "iopub.status.idle": "2021-01-27T22:57:45.025648Z",
     "shell.execute_reply": "2021-01-27T22:57:45.025150Z"
    },
    "origin_pos": 16,
    "slideshow": {
     "slide_type": "-"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "X = torch.normal(0, 1, (2, 5))\n",
    "X_prob = softmax(X)\n",
    "X_prob, X_prob.sum(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decent-hebrew",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Defining the Model\n",
    "\n",
    "Let's implement the softmax regression model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mental-grace",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Note that we flatten each original image in the batch into a vector using the reshape function before passing the data through our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pharmaceutical-motorcycle",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T22:57:45.029473Z",
     "iopub.status.busy": "2021-01-27T22:57:45.028907Z",
     "iopub.status.idle": "2021-01-27T22:57:45.031383Z",
     "shell.execute_reply": "2021-01-27T22:57:45.030819Z"
    },
    "origin_pos": 19,
    "slideshow": {
     "slide_type": "-"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    return softmax(torch.matmul(X.reshape((-1, W.shape[0])), W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indirect-individual",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Defining the Loss Function\n",
    "\n",
    "Next, we implement the cross-entropy loss function, one the most common loss function in deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "terminal-contributor",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T22:57:45.042708Z",
     "iopub.status.busy": "2021-01-27T22:57:45.042169Z",
     "iopub.status.idle": "2021-01-27T22:57:45.046328Z",
     "shell.execute_reply": "2021-01-27T22:57:45.045851Z"
    },
    "origin_pos": 24,
    "slideshow": {
     "slide_type": "-"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def cross_entropy(y_hat, y):\n",
    "    return - torch.log(y_hat[range(len(y_hat)), y])\n",
    "\n",
    "cross_entropy(y_hat, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vietnamese-caution",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Recall that cross-entropy takes the negative log-likelihood of the predicted probability assigned to the true label. Rather than iterating over the predictions with a Python for-loop (which tends to be inefficient), we can pick all elements by a single operator. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seventh-genesis",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Below, we create sample data y_hat with 2 examples of predicted probabilities over 3 classes and their corresponding labels y. With y we know that in the first example the first class is the correct prediction and in the second example the third class is the ground-truth. \n",
    "\n",
    "Using y as the indices of the probabilities in y_hat, we pick the probability of the first class in the first example and the probability of the third class in the second example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-angola",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T22:57:45.035589Z",
     "iopub.status.busy": "2021-01-27T22:57:45.035047Z",
     "iopub.status.idle": "2021-01-27T22:57:45.038354Z",
     "shell.execute_reply": "2021-01-27T22:57:45.038794Z"
    },
    "origin_pos": 21,
    "slideshow": {
     "slide_type": "notes"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "y = torch.tensor([0, 2])\n",
    "y_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])\n",
    "y_hat[[0, 1], y]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protective-reducing",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification Accuracy\n",
    "\n",
    "We use argmax to obtain the predicted class by the index for the largest entry in each row."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-canvas",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Given the predicted probability distribution y_hat, we typically choose the class with the highest predicted probability whenever we must output a hard prediction. Indeed, many applications require that we make a choice. Gmail must categorize an email into “Primary”, “Social”, “Updates”, or “Forums”. It might estimate probabilities internally, but at the end of the day it has to choose one among the classes.\n",
    "\n",
    "When predictions are consistent with the label class y, they are correct. The classification accuracy is the fraction of all predictions that are correct. Although it can be difficult to optimize accuracy directly (it is not differentiable), it is often the performance measure that we care most about, and we will nearly always report it when training classifiers.\n",
    "\n",
    "To compute accuracy we do the following. First, if y_hat is a matrix, we assume that the second dimension stores prediction scores for each class. We use argmax to obtain the predicted class by the index for the largest entry in each row. Then we compare the predicted class with the ground-truth y elementwise. Since the equality operator == is sensitive to data types, we convert y_hat’s data type to match that of y. The result is a tensor containing entries of 0 (false) and 1 (true). Taking the sum yields the number of correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaptive-breathing",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T22:57:45.051198Z",
     "iopub.status.busy": "2021-01-27T22:57:45.050657Z",
     "iopub.status.idle": "2021-01-27T22:57:45.052750Z",
     "shell.execute_reply": "2021-01-27T22:57:45.053186Z"
    },
    "origin_pos": 27,
    "slideshow": {
     "slide_type": "-"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def accuracy(y_hat, y):  #@save\n",
    "    \"\"\"Compute the number of correct predictions.\"\"\"\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "        y_hat = y_hat.argmax(axis=1)\n",
    "    cmp = y_hat.type(y.dtype) == y\n",
    "    return float(cmp.type(y.dtype).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-driving",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's calculate the accuracy from the previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mighty-cable",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T22:57:45.056787Z",
     "iopub.status.busy": "2021-01-27T22:57:45.056253Z",
     "iopub.status.idle": "2021-01-27T22:57:45.059495Z",
     "shell.execute_reply": "2021-01-27T22:57:45.059934Z"
    },
    "origin_pos": 29,
    "slideshow": {
     "slide_type": "-"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "accuracy(y_hat, y) / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foreign-portrait",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We can see that the first example’s prediction class is 2 (the largest element of the row is 0.6 with the index 2), which is inconsistent with the actual label, 0. The second example’s prediction class is 2 (the largest element of the row is 0.5 with the index of 2), which is consistent with the actual label, 2. Therefore, the classification accuracy rate for these two examples is 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anticipated-likelihood",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Similarly, we can evaluate the accuracy for any model or neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nearby-antenna",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T22:57:45.064724Z",
     "iopub.status.busy": "2021-01-27T22:57:45.064163Z",
     "iopub.status.idle": "2021-01-27T22:57:45.066201Z",
     "shell.execute_reply": "2021-01-27T22:57:45.066638Z"
    },
    "origin_pos": 32,
    "slideshow": {
     "slide_type": "-"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def evaluate_accuracy(net, data_iter):  #@save\n",
    "    \"\"\"Compute the accuracy for a model on a dataset.\"\"\"\n",
    "    if isinstance(net, torch.nn.Module):\n",
    "        net.eval()  # Set the model to evaluation mode\n",
    "    metric = Accumulator(2)  # No. of correct predictions, no. of predictions\n",
    "    for X, y in data_iter:\n",
    "        metric.add(accuracy(net(X), y), y.numel())\n",
    "    return metric[0] / metric[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sufficient-location",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "`Accumulator` is a utility class to accumulate sums over multiple variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual-expert",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In the above evaluate_accuracy function, we create 2 variables in the Accumulator instance for storing both the number of correct predictions and the number of predictions, respectively. Both will be accumulated over time as we iterate over the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "available-conversation",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T22:57:45.071999Z",
     "iopub.status.busy": "2021-01-27T22:57:45.071455Z",
     "iopub.status.idle": "2021-01-27T22:57:45.073554Z",
     "shell.execute_reply": "2021-01-27T22:57:45.073992Z"
    },
    "origin_pos": 34,
    "slideshow": {
     "slide_type": "skip"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "class Accumulator:  #@save\n",
    "    \"\"\"For accumulating sums over `n` variables.\"\"\"\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0] * n\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-institute",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Similarly, we can evaluate the accuracy for any model net on a dataset that is accessed via the data iterator data_iter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handled-satin",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Because we initialized the net model with random weights, the accuracy of this model should be close to random guessing, i.e., 0.1 for 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressive-bookmark",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T22:57:45.077329Z",
     "iopub.status.busy": "2021-01-27T22:57:45.076735Z",
     "iopub.status.idle": "2021-01-27T22:57:45.439079Z",
     "shell.execute_reply": "2021-01-27T22:57:45.439533Z"
    },
    "origin_pos": 36,
    "slideshow": {
     "slide_type": "-"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "evaluate_accuracy(net, test_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "differential-coordination",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confidential-stevens",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The training loop for softmax regression should look strikingly familiar if you read through our implementation of linear regression in Section 3.2. Here we refactor the implementation to make it reusable. First, we define a function to train for one epoch. Note that updater is a general function to update the model parameters, which accepts the batch size as an argument. It can be either a wrapper of the d2l.sgd function or a framework’s built-in optimization function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corrected-province",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T22:57:45.447604Z",
     "iopub.status.busy": "2021-01-27T22:57:45.447037Z",
     "iopub.status.idle": "2021-01-27T22:57:45.449618Z",
     "shell.execute_reply": "2021-01-27T22:57:45.448985Z"
    },
    "origin_pos": 39,
    "slideshow": {
     "slide_type": "-"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def train_epoch_ch3(net, train_iter, loss, updater):  #@save\n",
    "    \"\"\"The training loop defined in Chapter 3.\"\"\"\n",
    "    # Set the model to training mode\n",
    "    if isinstance(net, torch.nn.Module):\n",
    "        net.train()\n",
    "    # Sum of training loss, sum of training accuracy, no. of examples\n",
    "    metric = Accumulator(3)\n",
    "    for X, y in train_iter:\n",
    "        # Compute gradients and update parameters\n",
    "        y_hat = net(X)\n",
    "        l = loss(y_hat, y)\n",
    "        if isinstance(updater, torch.optim.Optimizer):\n",
    "            # Using PyTorch in-built optimizer & loss criterion\n",
    "            updater.zero_grad()\n",
    "            l.backward()\n",
    "            updater.step()\n",
    "            metric.add(float(l) * len(y), accuracy(y_hat, y),\n",
    "                       y.size().numel())\n",
    "        else:\n",
    "            # Using custom built optimizer & loss criterion\n",
    "            l.sum().backward()\n",
    "            updater(X.shape[0])\n",
    "            metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())\n",
    "    # Return training loss and training accuracy\n",
    "    return metric[0] / metric[2], metric[1] / metric[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporated-criterion",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Before showing the implementation of the training function, we define a utility class that plot data in animation. Again, it aims to simplify code in the rest of the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funded-noise",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T22:57:45.460602Z",
     "iopub.status.busy": "2021-01-27T22:57:45.457113Z",
     "iopub.status.idle": "2021-01-27T22:57:45.463189Z",
     "shell.execute_reply": "2021-01-27T22:57:45.462619Z"
    },
    "origin_pos": 42,
    "slideshow": {
     "slide_type": "slide"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "class Animator:  #@save\n",
    "    \"\"\"For plotting data in animation.\"\"\"\n",
    "    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n",
    "                 ylim=None, xscale='linear', yscale='linear',\n",
    "                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\n",
    "                 figsize=(3.5, 2.5)):\n",
    "        # Incrementally plot multiple lines\n",
    "        if legend is None:\n",
    "            legend = []\n",
    "        d2l.use_svg_display()\n",
    "        self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize)\n",
    "        if nrows * ncols == 1:\n",
    "            self.axes = [self.axes, ]\n",
    "        # Use a lambda function to capture arguments\n",
    "        self.config_axes = lambda: d2l.set_axes(\n",
    "            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n",
    "        self.X, self.Y, self.fmts = None, None, fmts\n",
    "\n",
    "    def add(self, x, y):\n",
    "        # Add multiple data points into the figure\n",
    "        if not hasattr(y, \"__len__\"):\n",
    "            y = [y]\n",
    "        n = len(y)\n",
    "        if not hasattr(x, \"__len__\"):\n",
    "            x = [x] * n\n",
    "        if not self.X:\n",
    "            self.X = [[] for _ in range(n)]\n",
    "        if not self.Y:\n",
    "            self.Y = [[] for _ in range(n)]\n",
    "        for i, (a, b) in enumerate(zip(x, y)):\n",
    "            if a is not None and b is not None:\n",
    "                self.X[i].append(a)\n",
    "                self.Y[i].append(b)\n",
    "        self.axes[0].cla()\n",
    "        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n",
    "            self.axes[0].plot(x, y, fmt)\n",
    "        self.config_axes()\n",
    "        display.display(self.fig)\n",
    "        display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "molecular-fancy",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The following training function then trains a model net on a training dataset accessed via train_iter for multiple epochs, which is specified by num_epochs. At the end of each epoch, the model is evaluated on a testing dataset accessed via test_iter. We will leverage the Animator class to visualize the training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-duration",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T22:57:45.469074Z",
     "iopub.status.busy": "2021-01-27T22:57:45.468518Z",
     "iopub.status.idle": "2021-01-27T22:57:45.471060Z",
     "shell.execute_reply": "2021-01-27T22:57:45.470484Z"
    },
    "origin_pos": 44,
    "slideshow": {
     "slide_type": "slide"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater):  #@save\n",
    "    \"\"\"Train a model (defined in Chapter 3).\"\"\"\n",
    "    animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 0.9],\n",
    "                        legend=['train loss', 'train acc', 'test acc'])\n",
    "    for epoch in range(num_epochs):\n",
    "        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)\n",
    "        test_acc = evaluate_accuracy(net, test_iter)\n",
    "        animator.add(epoch + 1, train_metrics + (test_acc,))\n",
    "    train_loss, train_acc = train_metrics\n",
    "    assert train_loss < 0.5, train_loss\n",
    "    assert train_acc <= 1 and train_acc > 0.7, train_acc\n",
    "    assert test_acc <= 1 and test_acc > 0.7, test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distributed-thread",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "As an implementation from scratch, we use the minibatch stochastic gradient descent defined in Section 3.2 to optimize the loss function of the model with a learning rate 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "based-return",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T22:57:45.474742Z",
     "iopub.status.busy": "2021-01-27T22:57:45.474197Z",
     "iopub.status.idle": "2021-01-27T22:57:45.476697Z",
     "shell.execute_reply": "2021-01-27T22:57:45.476227Z"
    },
    "origin_pos": 46,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "\n",
    "def updater(batch_size):\n",
    "    return d2l.sgd([W, b], lr, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaptive-zoning",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we train the model with 10 epochs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "orange-defense",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Note that both the number of epochs (num_epochs), and learning rate (lr) are adjustable hyperparameters. By changing their values, we may be able to increase the classification accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connected-satin",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T22:57:45.500925Z",
     "iopub.status.busy": "2021-01-27T22:57:45.481604Z",
     "iopub.status.idle": "2021-01-27T22:58:08.353193Z",
     "shell.execute_reply": "2021-01-27T22:58:08.353805Z"
    },
    "origin_pos": 49,
    "slideshow": {
     "slide_type": "-"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, updater)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confused-timothy",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standing-luxembourg",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Now that training is complete, our model is ready to classify some images. Given a series of images, we will compare their actual labels (first line of text output) and the predictions from the model (second line of text output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graduate-virginia",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T22:58:08.360913Z",
     "iopub.status.busy": "2021-01-27T22:58:08.359822Z",
     "iopub.status.idle": "2021-01-27T22:58:08.741418Z",
     "shell.execute_reply": "2021-01-27T22:58:08.742015Z"
    },
    "origin_pos": 51,
    "slideshow": {
     "slide_type": "-"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def predict_ch3(net, test_iter, n=6):  #@save\n",
    "    \"\"\"Predict labels (defined in Chapter 3).\"\"\"\n",
    "    for X, y in test_iter:\n",
    "        break\n",
    "    trues = d2l.get_fashion_mnist_labels(y)\n",
    "    preds = d2l.get_fashion_mnist_labels(net(X).argmax(axis=1))\n",
    "    titles = [true + '\\n' + pred for true, pred in zip(trues, preds)]\n",
    "    d2l.show_images(X[0:n].reshape((n, 28, 28)), 1, n, titles=titles[0:n])\n",
    "\n",
    "\n",
    "predict_ch3(net, test_iter)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "rise": {
   "autolaunch": true,
   "enable_chalkboard": true,
   "overlay": "<div class='my-top-right'><img height=80px src='http://d2l.ai/_static/logo-with-text.png'/></div><div class='my-top-left'></div>"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
