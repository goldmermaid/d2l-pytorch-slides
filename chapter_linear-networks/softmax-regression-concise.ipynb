{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "second-radius",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Concise Implementation of Softmax Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annoying-rouge",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T23:11:28.824126Z",
     "iopub.status.busy": "2021-01-27T23:11:28.823465Z",
     "iopub.status.idle": "2021-01-27T23:11:30.033195Z",
     "shell.execute_reply": "2021-01-27T23:11:30.032544Z"
    },
    "origin_pos": 2,
    "slideshow": {
     "slide_type": "-"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "from d2l import torch as d2l\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-cologne",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Similarly to section \"Concise Implementation of Linear Regression\", let's use PyTorch to train a softmax regression model.\n",
    "\n",
    "We will stick with the Fashion-MNIST dataset and keep the batch size at 256 as the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "covered-mixture",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T23:11:30.037267Z",
     "iopub.status.busy": "2021-01-27T23:11:30.036692Z",
     "iopub.status.idle": "2021-01-27T23:11:30.081906Z",
     "shell.execute_reply": "2021-01-27T23:11:30.081249Z"
    },
    "origin_pos": 4,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "presidential-influence",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "First, let's define our neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "municipal-sample",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T23:11:30.087178Z",
     "iopub.status.busy": "2021-01-27T23:11:30.086621Z",
     "iopub.status.idle": "2021-01-27T23:11:30.092992Z",
     "shell.execute_reply": "2021-01-27T23:11:30.092408Z"
    },
    "origin_pos": 7,
    "slideshow": {
     "slide_type": "-"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.Flatten(), nn.Linear(784, 10))\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, std=0.01)\n",
    "\n",
    "net.apply(init_weights);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-identity",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Recall the output layer of softmax regression is a fully-connected layer. Therefore, we just need to add one fully-connected layer with 10 outputs to our Sequential. \n",
    "\n",
    "we initialize the weights at random with zero mean and standard deviation 0.01. Let's print our neural net and see how it looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concrete-drink",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Note that PyTorch does not implicitly reshape the inputs. Thus we define the flatten\n",
    "layer to reshape the inputs before the linear layer in our network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposed-portugal",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's define the loss function and the optimization method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annoying-animation",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Rather than building everything from scratch, we will call the built-in loss function `CrossEntropyLoss` from PyTorch.\n",
    "\n",
    "Note here, instead of passing softmax probabilities into our new loss function, we will pass the logits and compute the softmax and its log all at once inside the cross-entropy loss function, which does smart things like the “LogSumExp trick”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raising-amber",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T23:11:30.096612Z",
     "iopub.status.busy": "2021-01-27T23:11:30.096072Z",
     "iopub.status.idle": "2021-01-27T23:11:30.098182Z",
     "shell.execute_reply": "2021-01-27T23:11:30.098625Z"
    },
    "origin_pos": 11,
    "slideshow": {
     "slide_type": "-"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threatened-presentation",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Similarly, for optimization method, we use minibatch stochastic gradient descent with a learning rate of 0.1 as the optimization algorithm. \n",
    "\n",
    "Note that this `SGD` function the same as we applied in the concise version of linear regression example. As you can see, the general applicability of the optimizers is well demonstrated here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neutral-morrison",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T23:11:30.102355Z",
     "iopub.status.busy": "2021-01-27T23:11:30.101805Z",
     "iopub.status.idle": "2021-01-27T23:11:30.104376Z",
     "shell.execute_reply": "2021-01-27T23:11:30.103804Z"
    },
    "origin_pos": 15,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "trainer = torch.optim.SGD(net.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chubby-rough",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Finally, we call the training function defined in the previous lecture to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "married-palmer",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T23:11:30.128350Z",
     "iopub.status.busy": "2021-01-27T23:11:30.121913Z",
     "iopub.status.idle": "2021-01-27T23:11:53.760736Z",
     "shell.execute_reply": "2021-01-27T23:11:53.761373Z"
    },
    "origin_pos": 18,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-gross",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Even though at this time, we have fewer lines of code than implement everything from scratch, this neural net converges to a solution that achieves a decent accuracy."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "rise": {
   "autolaunch": true,
   "enable_chalkboard": true,
   "overlay": "<div class='my-top-right'><img height=80px src='http://d2l.ai/_static/logo-with-text.png'/></div><div class='my-top-left'></div>"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
