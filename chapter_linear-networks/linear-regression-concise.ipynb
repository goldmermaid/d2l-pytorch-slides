{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "instant-mechanism",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Concise Implementation of Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "express-pharmacology",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In the last lecture, we showed you\n",
    "how to build a linear regression model from scratch.\n",
    "\n",
    "While In practice, there are more simple ways, since\n",
    "data iterators, loss functions, optimizers,\n",
    "and neural network layers\n",
    "are common used in a variety of deep learning models.\n",
    "\n",
    "Hence, In this lecture, we will show you how to implement \n",
    "the linear regression model from concisely by using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finished-mambo",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T20:26:27.342235Z",
     "iopub.status.busy": "2021-01-27T20:26:27.341590Z",
     "iopub.status.idle": "2021-01-27T20:26:28.543413Z",
     "shell.execute_reply": "2021-01-27T20:26:28.542632Z"
    },
    "origin_pos": 1,
    "slideshow": {
     "slide_type": "-"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "from d2l import torch as d2l\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historic-there",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generating the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promising-allowance",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "To start, we will generate the same dataset as last lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "macro-indonesia",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T20:26:28.548126Z",
     "iopub.status.busy": "2021-01-27T20:26:28.547566Z",
     "iopub.status.idle": "2021-01-27T20:26:28.550789Z",
     "shell.execute_reply": "2021-01-27T20:26:28.550206Z"
    },
    "origin_pos": 3,
    "slideshow": {
     "slide_type": "-"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "true_w = torch.tensor([2, -3.4])\n",
    "true_b = 4.2\n",
    "features, labels = d2l.synthetic_data(true_w, true_b, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-warrant",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reading the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intermediate-portfolio",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Rather than rolling our own iterator,\n",
    "we can call upon the existing API in a framework to read data.\n",
    "We pass in `features` and `labels` as arguments and specify `batch_size`\n",
    "when instantiating a data iterator object.\n",
    "Besides, the boolean value `is_train`\n",
    "indicates whether or not\n",
    "we want the data iterator object to shuffle the data\n",
    "on each epoch (pass through the dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sonic-destination",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T20:26:28.555157Z",
     "iopub.status.busy": "2021-01-27T20:26:28.554583Z",
     "iopub.status.idle": "2021-01-27T20:26:28.557245Z",
     "shell.execute_reply": "2021-01-27T20:26:28.556654Z"
    },
    "origin_pos": 5,
    "slideshow": {
     "slide_type": "-"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def load_array(data_arrays, batch_size, is_train=True):  #@save\n",
    "    \"\"\"Construct a PyTorch data iterator.\"\"\"\n",
    "    dataset = data.TensorDataset(*data_arrays)\n",
    "    return data.DataLoader(dataset, batch_size, shuffle=is_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numeric-motion",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T20:26:28.561058Z",
     "iopub.status.busy": "2021-01-27T20:26:28.560398Z",
     "iopub.status.idle": "2021-01-27T20:26:28.562820Z",
     "shell.execute_reply": "2021-01-27T20:26:28.562243Z"
    },
    "origin_pos": 7,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "data_iter = load_array((features, labels), batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "allied-assumption",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Now we can use `data_iter` in much the same way as we called\n",
    "the `data_iter` function in :numref:`sec_linear_scratch`.\n",
    "To verify that it is working, we can read and print\n",
    "the first minibatch of examples.\n",
    "Comparing with :numref:`sec_linear_scratch`,\n",
    "here we use `iter` to construct a Python iterator and use `next` to obtain the first item from the iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compact-norfolk",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T20:26:28.566427Z",
     "iopub.status.busy": "2021-01-27T20:26:28.565882Z",
     "iopub.status.idle": "2021-01-27T20:26:28.573590Z",
     "shell.execute_reply": "2021-01-27T20:26:28.574180Z"
    },
    "origin_pos": 9,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "next(iter(data_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cathedral-lingerie",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In PyTorch, the fully-connected layer is defined in the Linear class. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subjective-toronto",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Note that we passed two arguments into nn.Linear. The first one specifies the input feature dimension, which is 2, and the second one is the output feature dimension, which is a single scalar and therefore 1.\n",
    "\n",
    "`nn` is an abbreviation for neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annual-cigarette",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T20:26:28.578298Z",
     "iopub.status.busy": "2021-01-27T20:26:28.577750Z",
     "iopub.status.idle": "2021-01-27T20:26:28.580343Z",
     "shell.execute_reply": "2021-01-27T20:26:28.579762Z"
    },
    "origin_pos": 15,
    "slideshow": {
     "slide_type": "-"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "net = nn.Sequential(nn.Linear(2, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affected-function",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Initializing Model Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooked-sphere",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "As we have specified the input and output dimensions when constructing nn.Linear. Now we access the parameters directly to specify their initial values. We first locate the layer by net[0], which is the first layer in the network, and then use the weight.data and bias.data methods to access the parameters. Next we use the replace methods normal_ and fill_ to overwrite parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animal-purse",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T20:26:28.584527Z",
     "iopub.status.busy": "2021-01-27T20:26:28.583809Z",
     "iopub.status.idle": "2021-01-27T20:26:28.587509Z",
     "shell.execute_reply": "2021-01-27T20:26:28.586930Z"
    },
    "origin_pos": 21,
    "slideshow": {
     "slide_type": "-"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "net[0].weight.data.normal_(0, 0.01)\n",
    "net[0].bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "previous-confidentiality",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Defining the Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liked-shipping",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The MSELoss class computes the mean squared error, also known as squared $ùêø_2$ norm. By default it returns the average loss over examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-samuel",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T20:26:28.591118Z",
     "iopub.status.busy": "2021-01-27T20:26:28.590571Z",
     "iopub.status.idle": "2021-01-27T20:26:28.593097Z",
     "shell.execute_reply": "2021-01-27T20:26:28.592524Z"
    },
    "origin_pos": 31,
    "slideshow": {
     "slide_type": "-"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-duration",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Defining the Optimization Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "future-start",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Minibatch stochastic gradient descent is a standard tool for optimizing neural networks and thus PyTorch supports it alongside a number of variations on this algorithm in the optim module. When we instantiate an SGD instance, we will specify the parameters to optimize over (obtainable from our net via net.parameters()), with a dictionary of hyperparameters required by our optimization algorithm. Minibatch stochastic gradient descent just requires that we set the value lr, which is set to 0.03 here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-equipment",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T20:26:28.597009Z",
     "iopub.status.busy": "2021-01-27T20:26:28.596462Z",
     "iopub.status.idle": "2021-01-27T20:26:28.599046Z",
     "shell.execute_reply": "2021-01-27T20:26:28.598573Z"
    },
    "origin_pos": 37,
    "slideshow": {
     "slide_type": "-"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "trainer = torch.optim.SGD(net.parameters(), lr=0.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reasonable-manual",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guided-force",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "You might have noticed that expressing our model through high-level APIs of a deep learning framework requires comparatively few lines of code. We did not have to individually allocate parameters, define our loss function, or implement minibatch stochastic gradient descent. Once we start working with much more complex models, advantages of high-level APIs will grow considerably. However, once we have all the basic pieces in place, the training loop itself is strikingly similar to what we did when implementing everything from scratch.\n",
    "\n",
    "To refresh your memory: for some number of epochs, we will make a complete pass over the dataset (train_data), iteratively grabbing one minibatch of inputs and the corresponding ground-truth labels. For each minibatch, we go through the following ritual:\n",
    "\n",
    "- Generate predictions by calling net(X) and calculate the loss l (the forward propagation).\n",
    "\n",
    "- Calculate gradients by running the backpropagation.\n",
    "\n",
    "- Update the model parameters by invoking our optimizer.\n",
    "\n",
    "For good measure, we compute the loss after each epoch and print it to monitor progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "express-penny",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T20:26:28.603718Z",
     "iopub.status.busy": "2021-01-27T20:26:28.603167Z",
     "iopub.status.idle": "2021-01-27T20:26:28.773128Z",
     "shell.execute_reply": "2021-01-27T20:26:28.772554Z"
    },
    "origin_pos": 39,
    "slideshow": {
     "slide_type": "-"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter:\n",
    "        l = loss(net(X) ,y)\n",
    "        trainer.zero_grad()\n",
    "        l.backward()\n",
    "        trainer.step()\n",
    "    l = loss(net(features), labels)\n",
    "    print(f'epoch {epoch + 1}, loss {l:f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premium-samba",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Below, we compare the model parameters learned by training on finite data and the actual parameters that generated our dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "current-thailand",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "To access parameters, we first access the layer that we need from net and then access that layer‚Äôs weights and bias. As in our from-scratch implementation, note that our estimated parameters are close to their ground-truth counterparts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recorded-windows",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T20:26:28.777124Z",
     "iopub.status.busy": "2021-01-27T20:26:28.776576Z",
     "iopub.status.idle": "2021-01-27T20:26:28.780970Z",
     "shell.execute_reply": "2021-01-27T20:26:28.781446Z"
    },
    "origin_pos": 41,
    "slideshow": {
     "slide_type": "-"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "w = net[0].weight.data\n",
    "print('error in estimating w:', true_w - w.reshape(true_w.shape))\n",
    "b = net[0].bias.data\n",
    "print('error in estimating b:', true_b - b)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "rise": {
   "autolaunch": true,
   "enable_chalkboard": true,
   "overlay": "<div class='my-top-right'><img height=80px src='http://d2l.ai/_static/logo-with-text.png'/></div><div class='my-top-left'></div>"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
